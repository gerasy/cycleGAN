{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-b410f5a7-55df-4ad1-ae9b-50e0a0fa7873","deepnote_to_be_reexecuted":false,"source_hash":"92ec937a","execution_millis":899,"execution_start":1612173175355,"deepnote_cell_type":"code"},"source":"#-*- coding:utf-8 -*-\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-c7c27bbd-4107-449d-ac03-296fa88f53eb","deepnote_to_be_reexecuted":false,"source_hash":"462c77d0","execution_millis":57,"execution_start":1612173176298,"deepnote_cell_type":"code"},"source":"class CycleGAN(object):\n    def __init__(self, genA2B, genB2A, discA, discB, classifier, device):\n        self.genA2B = genA2B\n        self.genB2A = genB2A\n        self.discA = discA\n        self.discB = discB\n        self.classifier = classifier\n        self.device = device\n        self.cycle_trained = False\n        self.classifier_trained = False\n\n    def train(self,param):\n\n        self.genA2B.apply(self.init_weights).to(self.device)\n        self.genB2A.apply(self.init_weights).to(self.device)\n        self.discA.apply(self.init_weights).to(self.device)\n        self.discB.apply(self.init_weights).to(self.device)\n        \n        name = param.name\n        input_nc_T = param.channels    # input channels\n        output_nc_T = param.channels   # output channels\n        epoch_T = 0       # starting epoch\n        n_epochs_T = param.epochs   # number of epochs of training\n        decay_epoch_T = np.ceil(n_epochs_T / param.lr_sched) # epoch to start linearly decaying the learning rate \n        lr_T = param.lr     # initial learning rate\n        size_T = param.size        # image size (width or height), squared assumed\n        batchSize_T = param.bs           # batchsize\n        lambda_iden = param.lambdas[1]   # eig 5\n        lambda_cyc  = param.lambdas[0]   # eig 10\n        size_replay_buffer = param.size_replay_buffer\n        resnet_blocks = param.resnet_blocks\n        loss_adv = param.loss_adv\n        loss_cyc_ide = param.loss_cyc_ide\n        down_upsampling_layers = param.down_upsampling_layers\n\n    \n        cuda = torch.cuda.is_available()\n        device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n        print(\"using cuda: \", cuda)\n\n        # inputs and targets memory allocation\n        Tensor = torch.Tensor\n        input_A_T = Tensor(batchSize_T, input_nc_T, size_T, size_T)\n        input_B_T = Tensor(batchSize_T, output_nc_T, size_T, size_T)\n        target_real_T = torch.ones((1),requires_grad=False).to(device) \n        target_fake_T = torch.zeros((1),requires_grad=False).to(device)\n\n        # init replayBuffer\n        fake_A_buffer_T = ReplayBuffer(param.size_replay_buffer)\n        fake_B_buffer_T = ReplayBuffer(param.size_replay_buffer)\n\n        # init networks\n        netG_A2B_T = Small_Generator(input_nc_T, output_nc_T,n_residual_blocks=param.resnet_blocks,down_upsampling_layers=param.down_upsampling_layers)\n        netG_B2A_T = Small_Generator(output_nc_T, input_nc_T,n_residual_blocks=param.resnet_blocks,down_upsampling_layers=param.down_upsampling_layers)\n        netD_A_T = Small_Discriminator(input_nc_T)\n        netD_B_T = Small_Discriminator(output_nc_T)\n\n        #init losses\n        losses = {\"epoch\": [], \"adv_G_A2B\": [],\"adv_G_B2A\": [],\"adv_D_A\": [], \"adv_D_B\": [], \"cycle_loss\": [], \"identity_loss\": []}\n\n        # init weights and putting them to device\n        netG_A2B_T.apply(weights_init_normal).to(device)\n        netG_B2A_T.apply(weights_init_normal).to(device)\n        netD_A_T.apply(weights_init_normal).to(device)\n        netD_B_T.apply(weights_init_normal).to(device)\n\n        # define lossfunctions\n        criterion_GAN_T = param.loss_adv\n        criterion_cycle_T = param.loss_cyc_ide\n        criterion_identity_T = param.loss_cyc_ide\n\n\n        # define optimizers\n        optimizer_G_T = torch.optim.Adam(itertools.chain(netG_A2B_T.parameters(), netG_B2A_T.parameters()), lr=lr_T, betas=(0.5, 0.999))\n        optimizer_D_A_T = torch.optim.Adam(netD_A_T.parameters(), lr=lr_T, betas=(0.5, 0.999))\n        optimizer_D_B_T = torch.optim.Adam(netD_B_T.parameters(), lr=lr_T, betas=(0.5, 0.999))\n\n        # define learning rate schedulers\n        lr_sched_G   = torch.optim.lr_scheduler.LambdaLR(optimizer_G_T, lr_lambda=LambdaLR(n_epochs_T, epoch_T, decay_epoch_T).step)\n        lr_sched_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A_T, lr_lambda=LambdaLR(n_epochs_T, epoch_T, decay_epoch_T).step)\n        lr_sched_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B_T, lr_lambda=LambdaLR(n_epochs_T, epoch_T, decay_epoch_T).step)\n\n        # define transformations for data\n        if input_ch == 1:\n            transforms_T = [\n                transforms.Resize(int(size_T*1.12), Image.BICUBIC), \n                transforms.RandomCrop(size_T), \n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5), (0.5))\n            ]\n        else:\n            transforms_T = [\n                transforms.Resize(int(size_T*1.12), Image.BICUBIC), \n                transforms.RandomCrop(size_T), \n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ]\n\n        \n        # create dataloader\n        train_dataset_T = ImageDataset(pathA_Train, pathB_Train, transforms_ = transforms_T,  unaligned=True, rgb = False)\n        train_loader_T = DataLoader(train_dataset_T, batch_size=batchSize_T, shuffle = True) \n\n        # putting all nets to training mode\n        netG_A2B_T.train()\n        netG_B2A_T.train()\n        netD_A_T.train()\n        netD_B_T.train()    \n\n        for epoch in range(epoch_T,n_epochs_T):\n            tic = time.time()\n            for i, batch in enumerate(train_loader_T):\n                inner_tic = time.time()\n                # model input\n                real_A = Variable(input_A_T.copy_(batch['A'])).to(device)\n                real_B = Variable(input_B_T.copy_(batch['B'])).to(device)\n\n                ###### Train Generators A2B and B2A #####\n                optimizer_G_T.zero_grad()\n\n                # GAN (adversial) loss\n                # aka loss is small, if disc thiks generated sample looks real\n                fake_B = netG_A2B_T(real_A)\n                loss_GAN_A2B = criterion_GAN_T(netD_B_T(fake_B), target_real_T.expand_as(netD_B_T(fake_B)))\n                fake_A = netG_B2A_T(real_B)\n                loss_GAN_B2A = criterion_GAN_T(netD_A_T(fake_A), target_real_T.expand_as(netD_A_T(fake_A)))\n                \n                gan_loss = loss_GAN_A2B + loss_GAN_B2A\n\n                # Cycle loss\n                # aka loss is small if recovered image is similar to original \n                recovered_A = netG_B2A_T(fake_B)\n                loss_cycle_ABA = criterion_cycle_T(recovered_A, real_A) \n                recovered_B = netG_A2B_T(fake_A)\n                loss_cycle_BAB = criterion_cycle_T(recovered_B, real_B) \n                \n                cycle_loss = loss_cycle_ABA + loss_cycle_BAB\n                \n                # Identity loss\n                # G_A2B(B) should equal B if real B is fed\n                same_B = netG_A2B_T(real_B)\n                loss_identity_B = criterion_identity_T(same_B, real_B) \n                same_A = netG_B2A_T(real_A)\n                loss_identity_A = criterion_identity_T(same_A, real_A) \n                \n                identity_loss = loss_identity_A +loss_identity_B   \n            \n                # Total loss\n                loss_G = gan_loss + (identity_loss * lambda_iden) + (cycle_loss * lambda_cyc)\n                loss_G.backward()\n\n                optimizer_G_T.step()\n\n                ###### Train Discriminator A ######\n                optimizer_D_A_T.zero_grad()\n\n                # Real loss\n                pred_real = netD_A_T(real_A)\n                loss_D_real = criterion_GAN_T(pred_real, target_real_T.expand_as(pred_real))\n\n                # Fake loss using a image buffer\n                fake_A = fake_A_buffer_T.push_and_pop(fake_A)\n                pred_fake = netD_A_T(fake_A.detach())\n                loss_D_fake = criterion_GAN_T(pred_fake, target_fake_T.expand_as(pred_fake))\n\n                # Total loss\n                loss_D_A = (loss_D_real + loss_D_fake) / 2\n                loss_D_A.backward()\n\n                optimizer_D_A_T.step()\n\n                ###### Train Discriminator B #####\n                optimizer_D_B_T.zero_grad()\n\n                # Real loss\n                pred_real = netD_B_T(real_B)\n                loss_D_real = criterion_GAN_T(pred_real, target_real_T.expand_as(pred_real))\n                \n                # Fake loss\n                fake_B = fake_B_buffer_T.push_and_pop(fake_B)\n                pred_fake = netD_B_T(fake_B.detach())\n                loss_D_fake = criterion_GAN_T(pred_fake, target_fake_T.expand_as(pred_fake))\n\n                # Total loss\n                loss_D_B = (loss_D_real + loss_D_fake) / 2\n                loss_D_B.backward()\n\n                optimizer_D_B_T.step()\n\n                inner_tac = time.time()\n                \n                losses[\"epoch\"].append(epoch)\n                losses[\"adv_G_A2B\"].append(loss_GAN_A2B.detach().cpu().numpy())\n                losses[\"adv_G_B2A\"].append(loss_GAN_B2A.detach().cpu().numpy())\n                losses[\"adv_D_A\"].append(loss_D_A.detach().cpu().numpy())\n                losses[\"adv_D_B\"].append(loss_D_B.detach().cpu().numpy())\n                losses[\"cycle_loss\"].append(cycle_loss.detach().cpu().numpy())\n                losses[\"identity_loss\"].append(identity_loss.detach().cpu().numpy())\n\n                print(\"batch {} done in {} seconds, cycle_loss:{}\".format(i+1,np.round(inner_tac-inner_tic, decimals = 4),cycle_loss))\n\n            # save the last model\n            if (epoch==n_epochs_T-1) :\n                save_model(netG_A2B_T, checkpointsFolder, \"netG_A2B_MNIST\", epoch+1, param)\n                save_model(netG_B2A_T, checkpointsFolder, \"netG_B2A_MNIST\", epoch+1, param)\n                save_model(netD_A_T, checkpointsFolder, \"netD_A_MNIST\", epoch+1, param)\n                save_model(netD_B_T, checkpointsFolder, \"netD_B_MNIST\", epoch+1, param)\n            \n            # save losses per epoch\n            losses[\"epoch\"].append(epoch)\n            losses[\"adv_G_A2B\"].append(loss_GAN_A2B.detach().cpu().numpy())\n            losses[\"adv_G_B2A\"].append(loss_GAN_B2A.detach().cpu().numpy())\n            losses[\"adv_D_A\"].append(loss_D_A.detach().cpu().numpy())\n            losses[\"adv_D_B\"].append(loss_D_B.detach().cpu().numpy())\n            losses[\"cycle_loss\"].append(cycle_loss.detach().cpu().numpy())\n            losses[\"identity_loss\"].append(identity_loss.detach().cpu().numpy())\n            # backup losses\n            np.save(checkpointsFolder+\"lossesMNIST_{}.npy\".format(param.name), losses)     \n            tac = time.time()\n            print(\"epoch {} of {} finished in {} seconds, cycle_loss: {}\".format(epoch+1,n_epochs_T, \n                                                                                np.round(tac-tic, decimals = 3), (cycle_loss))) \n\n            # update learning rates\n            lr_sched_G.step()\n            lr_sched_D_A.step()\n            lr_sched_D_B.step()\n\n        \n\n\n    def train_classifier(self, param):\n        if self.classifier_trained == True:\n            print(\"classifier already trained\")\n        \n\n    def eval(self, img, target_domain): \n    # eval function to map an image to target domain\n    # and give a certantinty \"quality\"-measure of the result\n        softy = nn.Softmax(dim = 1)\n        if target_domain == \"A\":\n            gen_img   = self.genB2A(img)\n            certantiy = np.round(softy(self.classifier(img).detach()).cpu().numpy(), decimals = 3)[1]\n        elif target_domain == \"B\":\n            result = self.genA2B(img)\n            certantiy = np.round(softy(self.classifier(img).detach()).cpu().numpy(), decimals = 3)[0]\n        else: \n            print(\"ALARM\") #t.b.a. vllt ne echte exception\n        return result, certantiy        \n\n    #def continue_train():\n        # hier richtige parameter wie aktuelle lr und so beachten\n        # vllt doch nur eine train funktion fuer für das cycleGan die etwas flexibler ist, statt train und countiue train?\n    \n   \n    def init_weights(self, m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n        elif classname.find('BatchNorm2d') != -1:\n            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n            torch.nn.init.costant_(m.bias.data, 0.0)\n\n    #def save_model():\n\n\n    #def load_model():\n        \n        ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-f780c379-aa1a-4186-97c7-7968f9a9a541","deepnote_to_be_reexecuted":false,"source_hash":"15d2f428","execution_millis":6,"execution_start":1612173176359,"deepnote_cell_type":"code"},"source":"class Generator(nn.Module):\n    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n        super(Generator, self).__init__()\n\n        # Initial convolution block       \n        model = [   nn.ReflectionPad2d(3),\n                    nn.Conv2d(input_nc, 64, 7),\n                    nn.InstanceNorm2d(64),\n                    nn.ReLU(inplace=True) ]\n\n        # Downsampling\n        in_features = 64\n        out_features = in_features*2\n        for _ in range(2):\n            model += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                        nn.InstanceNorm2d(out_features),\n                        nn.ReLU(inplace=True) ]\n            in_features = out_features\n            out_features = in_features*2\n\n        # Residual blocks\n        for _ in range(n_residual_blocks):\n            model += [ResidualBlock(in_features)]\n\n        # Upsampling\n        out_features = in_features//2\n        for _ in range(2):\n            model += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n                        nn.InstanceNorm2d(out_features),\n                        nn.ReLU(inplace=True) ]\n            in_features = out_features\n            out_features = in_features//2\n\n        # Output layer\n        model += [  nn.ReflectionPad2d(3),\n                    nn.Conv2d(64, output_nc, 7),\n                    nn.Tanh() ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-b2ac77ac-3d6a-4857-96b7-2fdafadb3d13","deepnote_to_be_reexecuted":false,"source_hash":"d9914e78","execution_millis":7,"execution_start":1612173176373,"deepnote_cell_type":"code"},"source":"class Small_Generator(nn.Module):\n    def __init__(self, input_nc, output_nc, n_residual_blocks=3, down_upsampling_layers = 2):\n        super(Small_Generator, self).__init__()\n\n        # Initial convolution block\n        n = 32\n        model = [   nn.ReflectionPad2d(3),\n                    nn.Conv2d(input_nc, n, 7),\n                    nn.InstanceNorm2d(n),\n                    nn.ReLU(inplace=True) ]\n\n        # Downsampling\n        depth = down_upsampling_layers #not bigger than 4!\n        in_features = n\n        out_features = in_features*2\n        for _ in range(depth):\n            model += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                        nn.InstanceNorm2d(out_features),\n                        nn.ReLU(inplace=True) ]\n            in_features = out_features\n            out_features = in_features*2\n\n        # Residual blocks\n        for _ in range(n_residual_blocks):\n            model += [ResidualBlock(in_features)]\n\n        # Upsampling\n        out_features = in_features//2\n        for _ in range(depth):\n            model += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n                        nn.InstanceNorm2d(out_features),\n                        nn.ReLU(inplace=True) ]\n            in_features = out_features\n            out_features = in_features//2\n\n        # Output layer\n        model += [  nn.ReflectionPad2d(3),\n                    nn.Conv2d(n, output_nc, 7),\n                    nn.Tanh() ]\n\n        self.model = nn.Sequential(*model)\n\n    def forwassrd(self, x):\n        return self.model(x)\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-ca125a46-556c-453b-9ce5-ebd1e1561a65","deepnote_to_be_reexecuted":false,"source_hash":"8799029f","execution_millis":52,"execution_start":1612173176381,"deepnote_cell_type":"code"},"source":"class Discriminator(nn.Module):\n    def __init__(self, input_nc):\n        super(Discriminator, self).__init__()\n\n        # A bunch of convolutions one after another\n        model = [   nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        model += [  nn.Conv2d(64, 128, 4, stride=2, padding=1),\n                    nn.InstanceNorm2d(128), \n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        model += [  nn.Conv2d(128, 256, 4, stride=2, padding=1),\n                    nn.InstanceNorm2d(256), \n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        model += [  nn.Conv2d(256, 512, 4, padding=1),\n                    nn.InstanceNorm2d(512), \n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        # FCN classification layer\n        model += [nn.Conv2d(512, 1, 4, padding=1)]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        x =  self.model(x)\n        return  x ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-95178a08-a8f6-4b5e-ad3d-1d2339432239","deepnote_to_be_reexecuted":false,"source_hash":"f8e18146","execution_millis":0,"execution_start":1612173176433,"deepnote_cell_type":"code"},"source":"class Small_Discriminator(nn.Module):\n    def __init__(self, input_nc):\n        super(Small_Discriminator, self).__init__()\n\n        # A bunch of convolutions one after another\n        n = 32\n        model = [   nn.Conv2d(input_nc, n, 4, stride=2, padding=1),\n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        model += [  nn.Conv2d(n, 2*n, 4, stride=2, padding=1),\n                    nn.InstanceNorm2d(2*n), \n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        model += [nn.Conv2d(2*n, 1, 2, padding=1)]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        x =  self.model(x)\n       \n        return  x ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-610e5a81-1064-4b39-ab35-0484e3a25acc","deepnote_to_be_reexecuted":false,"source_hash":"32eb5630","execution_millis":0,"execution_start":1612173176434,"deepnote_cell_type":"code"},"source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n\n        conv_block = [  nn.ReflectionPad2d(1),\n                        nn.Conv2d(in_features, in_features, 3),\n                        nn.InstanceNorm2d(in_features),\n                        nn.ReLU(inplace=True),\n                        nn.ReflectionPad2d(1),\n                        nn.Conv2d(in_features, in_features, 3),\n                        nn.InstanceNorm2d(in_features)  ]\n\n        self.conv_block = nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-1abf8940-a878-4014-ab4b-ec2a8163cb08","deepnote_to_be_reexecuted":false,"source_hash":"1ab9f325","execution_millis":43,"execution_start":1612173176434,"deepnote_cell_type":"code"},"source":"class Param():\n    def __init__(self, channels, epochs, size, name=\"default\", down_upsampling_layers=2, lr = 0.0002, lr_sched = 2.0, size_replay_buffer = 50, resnet_blocks =9, loss_adv = torch.nn.MSELoss(), loss_cyc_ide = torch.nn.L1Loss() , lambdas=(10,0.5), bs=1):\n        self.channels = channels\n        self.epochs = epochs\n        self.size = size\n        self.name = name #which param and which value\n        self.lr = lr\n        self.lr_sched = lr_sched\n        self.size_replay_buffer = size_replay_buffer\n        self.resnet_blocks = resnet_blocks\n        self.loss_adv = loss_adv\n        self.loss_cyc_ide = loss_cyc_ide\n        self.lambdas = lambdas\n        self.bs = bs\n        self.down_upsampling_layers = down_upsampling_layers\n    ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-007b6309-3879-4a15-acaf-d5867cac37e0","deepnote_to_be_reexecuted":false,"source_hash":"c9e800f9","execution_millis":0,"execution_start":1612173176478,"deepnote_cell_type":"code"},"source":"class LambdaLR():\n    def __init__(self, n_epochs, offset, decay_start_epoch):\n        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch\n\n    def step(self, epoch):\n        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-e097fcbd-e668-43c4-a17e-78283741f36f","deepnote_to_be_reexecuted":false,"source_hash":"c1bceb80","execution_millis":22,"execution_start":1612173176478,"deepnote_cell_type":"code"},"source":"class ReplayBuffer():\n    def __init__(self, max_size=50):\n        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data):\n        to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0,1) > 0.5:\n                    i = random.randint(0, self.max_size-1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return Variable(torch.cat(to_return))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-3313e2d2-6964-43bb-ae89-42c5d0169d8f","deepnote_to_be_reexecuted":false,"source_hash":"b85992cf","execution_millis":1,"execution_start":1612173176500,"deepnote_cell_type":"code"},"source":"class ImageDataset(Dataset):\n    def __init__(self, pathA, pathB,transforms_ = None, unaligned=False,rgb = True ):\n        self.pathA = pathA\n        self.pathB = pathB\n        self.unaligned = unaligned\n        self.rgb = rgb\n        #dont do transformation if there are no transforms..\n        if(transforms_==None):\n            self.dontTransform = True\n        else:\n            self.transform = transforms.Compose(transforms_)\n            self.dontTransform = False\n\n    def __len__(self):\n        return max(len(listdir(self.pathA)), len(listdir(self.pathB)))\n    \n        \n    def __getitem__(self, index):\n        sampleA = Image.open(self.pathA + listdir(self.pathA)[index % len(listdir(self.pathA))])\n        if self.unaligned:\n            sampleB = Image.open(self.pathB + listdir(self.pathB)[random.randint(0, len(listdir(self.pathB))-1)])\n        else:\n            sampleB = Image.open(self.pathB + listdir(self.pathB)[index % len(listdir(self.pathB))])\n        \n        #transform image AND convert to RGB to fix grayscale image dimension problem\n        if self.rgb:\n            sampleA = sampleA.convert('RGB')\n            sampleB = sampleB.convert('RGB')\n        #dont do transformation if there are no transforms..\n        if not self.dontTransform:\n            sampleA = self.transform(sampleA)\n            sampleB = self.transform(sampleB)     \n     \n        return {'A': sampleA, 'B': sampleB}","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-ecc45e5d-4442-435e-a073-d24890c5ba9d","deepnote_to_be_reexecuted":false,"source_hash":"dc507f49","execution_millis":0,"execution_start":1612173176501,"deepnote_cell_type":"code"},"source":"class Classifier():\n    ","execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"unexpected EOF while parsing (<ipython-input-12-867d0c768c0f>, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-867d0c768c0f>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-b040be7a-026b-4e27-b419-80e3e3b8df46","deepnote_to_be_reexecuted":true,"source_hash":"9521603e","execution_millis":1,"deepnote_cell_type":"code"},"source":"class UnNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return tensor","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00013-438a9498-95ee-4ec2-b02e-6a349c35247e","deepnote_to_be_reexecuted":true,"source_hash":"b623e53d","deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"2650d3ee-52f5-4f44-b731-ac2ae5a60615","deepnote_execution_queue":[],"deepnote":{}}}